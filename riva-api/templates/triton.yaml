# Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


{{- $root := . }}

{{- if .Values.modelRepoGenerator.useSeparateTriton }}
{{- range $name, $config := .Values.modelRepoGenerator.ngcModelConfigs }}
{{- if $config.enabled }}
{{- with $ -}}
{{- $model_list := $config.models }}
{{- $group_name := lower $name }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ $group_name }}
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ $group_name }}
    chart: {{ template "riva-server.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: "8002"
    prometheus.io/path: "/metrics"
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http
      name: triton-http
    - port: 8001
      targetPort: grpc
      name: triton-grpc
    - port: 8002
      targetPort: metrics
      name: triton-metrics
  selector:
    app: {{ $group_name }}
    release: {{ .Release.Name }}
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap-{{ $group_name }}
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ template "riva-server.name" . }}
    chart: {{ template "riva-server.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  download-and-deploy-models.sh: |
    #!/bin/bash
    download_and_deploy_ngc_models $@ 2>&1 | tee /dev/shm/{{ $group_name }}.log
    num_models_deployed=$(grep -e "skipping deployment" -e "Extract_binaries" /dev/shm/{{ $group_name }}.log | grep "/data/models" | wc -l)
    if [[ $num_models_deployed -eq 0 ]]
    then
      echo "Model deploy failed"
      exit -1
    fi

  run-triton.sh: |
    #!/bin/bash
    set -x
    models_to_load=$(grep -e "skipping deployment" -e "Extract_binaries" /dev/shm/{{ $group_name }}.log | grep "/data/models" | rev | cut -f 1 -d ' ' | rev | sed -e 's#/data/models/#--load-model=#' -e 's#/1##' | paste -s -d ' ')
    tritonserver --model-repository=${TRTIS_MODEL_STORE} --model-control-mode=explicit --allow-metrics=true --allow-gpu-metrics=true --allow-cpu-metrics=true --model-load-thread-count=8 ${models_to_load}

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ $group_name }}
  namespace: {{ .Release.Namespace }}
  labels:
    app: {{ $group_name }}
    chart: {{ template "riva-server.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
spec:
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  selector:
    matchLabels:
      app: {{ $group_name }}
      release: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ $group_name }}
        release: {{ .Release.Name }}

    spec:
      {{- if .Values.persistentVolumeClaim.usePVC }}
      securityContext:
        runAsGroup: 1000
        runAsUser: 1000
      {{- end }}
      nodeSelector:
          image: nvcr.io/nvidia/tritonserver:22.11-py3
          imagePullPolicy: {{ .Values.riva.pullPolicy }}
          {{- if not .Values.riva.visibleGpus }}
          resources:
            limits:
              nvidia.com/gpu: {{ .Values.riva.numGpus }}
          {{- end }}

          command: ["/etc/run-triton.sh"]

          env:
            - name: TRTIS_MODEL_STORE
              value: "/data/models"
            - name: LD_PRELOAD
              value: "{{- join ":" .Values.riva.trtPlugins }}"
            {{- if .Values.riva.visibleGpus }}
            - name: NVIDIA_VISIBLE_DEVICES
              value: {{ .Values.riva.visibleGpus | quote }}
            {{- end }}
            {{- if .Values.rivaEnterprise.enabled }}
            - name: RIVA_API_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.rivaEnterprise.ngcSecret }}
                  key: {{ .Values.ngcCredentials.ngcSecretKey }}
            - name: RIVA_API_NGC_ORG
              value: {{ .Values.rivaEnterprise.ngcOrg }}
            - name: RIVA_EULA
              value: {{- if .Values.rivaEnterprise.acceptEula}}"accept"{{- else }}"decline"{{- end }}
            {{- end }}

          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: grpc
            - containerPort: 8002
              name: metrics
          readinessProbe:
            httpGet:
              path: /v2/health/ready
              port: http
            initialDelaySeconds: 10
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: http
            initialDelaySeconds: 10
          startupProbe:
            httpGet:
              path: /v2/health/ready
              port: http
            initialDelaySeconds: 10
            failureThreshold: 12
            periodSeconds: 10
          volumeMounts:
            - mountPath: /data/
              name: workdir
            - mountPath: /dev/shm
              name: shm
            - mountPath: /etc/run-triton.sh
              name: configmap-{{ $group_name }}
              subPath: run-triton.sh
              readOnly: true
      imagePullSecrets:
        - name: {{ .Values.modelRepoGenerator.imagePullSecret }}

      {{- if (ne (len .Values.modelRepoGenerator.ngcModelConfigs) 0) }}
      initContainers:
        {{- if .Values.compatGpuCheckTimeout }}
        - name: riva-gpu-check
          image: {{ template "servicemaker_image" . }}
          imagePullPolicy: {{ .Values.modelRepoGenerator.pullPolicy }}
          command: [sh]
          args: [-c, "timeout $GPU_CHECK_TIMEOUT bash -c 'until ( nvidia-smi > /dev/null ) ;do sleep 10; echo Checking GPU visibility... ;done;echo GPU found'"]
          env:
            - name: GPU_CHECK_TIMEOUT
              value: {{ .Values.compatGpuCheckTimeout }}
          {{- if .Values.riva.visibleGpus }}
            - name: NVIDIA_VISIBLE_DEVICES
              value: {{ .Values.riva.visibleGpus | quote }}
          {{- end }}
          {{- if not .Values.riva.visibleGpus }}
          resources:
            limits:
              nvidia.com/gpu: 1
          {{- end }}
        {{- end }}

        - name: riva-model-init
          image: {{ template "servicemaker_image" . }}
          imagePullPolicy: {{ .Values.modelRepoGenerator.pullPolicy }}
          {{- if not .Values.riva.visibleGpus }}
          resources:
            limits:
              nvidia.com/gpu: {{ .Values.riva.numGpus }}
          {{- end }}
          volumeMounts:
            - name: workdir
              mountPath: /data/
            - mountPath: /dev/shm
              name: shm
            - mountPath: /etc/download-and-deploy-models.sh
              name: configmap-{{ $group_name }}
              subPath: download-and-deploy-models.sh
              readOnly: true
          command: ["/etc/download-and-deploy-models.sh"]
          args:
            {{- if .Values.modelRepoGenerator.overwriteRMIRS }}
            - "-d"
            {{- end }}
            {{- if .Values.modelRepoGenerator.overwriteModels }}
            - "-x"
            {{- end }}
            {{- range $model := $model_list }}
            - "{{ . }}"
            {{- end }}
          env:
            - name: NGC_CLI_ORG
              value: "nvidia"
            - name: NGC_CLI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.modelRepoGenerator.ngcSecret }}
                  key: {{ .Values.modelRepoGenerator.ngcSecretKey }}
            - name: MODEL_DEPLOY_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.modelRepoGenerator.modelDeploySecret }}
                  key: key
            {{- if .Values.riva.visibleGpus }}
            - name: NVIDIA_VISIBLE_DEVICES
              value: {{ .Values.riva.visibleGpus | quote }}
            {{- end }}
      {{- end }}

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
        - name: workdir
          {{- if .Values.persistentVolumeClaim.usePVC }}
          persistentVolumeClaim:
            claimName: {{ .Values.persistentVolumeClaim.workdirClaimName }}
          {{- else}}
          {{- if .Values.modelRepoGenerator.modelDeployVolume }}
          hostPath:
            type: DirectoryOrCreate
            path: {{ .Values.modelRepoGenerator.modelDeployVolume.hostPath.path }}
          {{- else }}
          emptyDir: {}
          {{- end }}
          {{- end}}
        - name: configmap-{{ $group_name }}
          configMap:
            name: configmap-{{ $group_name }}
            defaultMode: 0777

---
{{- end }}
{{- end }}
{{- end }}
{{- end }}
