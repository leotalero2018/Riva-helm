# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

replicaCount: 1

imageurl: "nvcr.io"
riva_ngc_org: "nvidia"
riva_ngc_team: "riva"
riva_ngc_image_version: "2.14.0"
riva_ngc_model_version: "2.14.0"

#for non ngc based images or custom images.
rawServerImage: ""
rawServicemakerImage: ""

riva:
  speechImageName: riva-speech
  pullPolicy: IfNotPresent
  numGpus: 1  #changing this is currently experimental and may have undefined behaviour
  # visibleGpus Used to explicitly specify GPU device(s) to use for deployment. For example, set this to "1" to use GPU at index 1 for deployment.
  # Specifying multiple GPUs here may have undefiend behaviour.
  visibleGpus: ""

rivaEnterprise:
  ngcSecret: rivadeploysecret
  ngcOrg: your_org_name
  acceptEula: false
  enabled: false

ngcCredentials:
  registry: nvcr.io
  username: $oauthtoken
  password: "ZWkwcnVxNHBlZGlidWZlaHNqZGRzMjY2bDE6ZjVhMTI0NGMtYWI4Zi00YzVjLThmODctYWRlNDE5NmY2NGE5"
  email: "leo.20waw@gmail.com"

# compatGpuCheckTimeout timeout for GPU ready check.
# Set to empty to disable GPU availablity check.
compatGpuCheckTimeout: ""

modelRepoGenerator:
  # k8s secrets required for connecting to NGC for model and container artifacts
  imagePullSecret: imagepullsecret
  ngcSecret: modelpullsecret
  ngcSecretKey: apikey
  modelDeploySecret: riva-model-deploy-key
  # Generated as `echo -n tlt_encode | base64 -w0`
  modelDeployKey: dGx0X2VuY29kZQ==

  # container to use for generating the model repositories
  imageName: riva-speech
  pullPolicy: IfNotPresent

  # force redownload of artifacts
  overwriteRMIRS: false
  # rebuild inference engines
  overwriteModels: true
  # clear artifacts and models before downloading and deploying
  clearAllRMIRSAndModels: true

  # useSeparateTriton when false : Riva server and single Triton server are run in single container
  # useSeparateTriton when true : Riva server and Triton server(s) are deployed in separate pods
  # When set to true, number of Triton server pods and models loaded on each Triton server is controlled
  # via groups under ngcModelConfigs config.
  useSeparateTriton: false

  # tritonTimeoutSeconds Applicable when useSeparateTriton is true. Controls the duration for which
  # Riva server waits for Triton to become ready.
  tritonTimeoutSeconds: "3600"

  # Model Repo Generator config files stored in NGC
  # If useSeparateTriton is false and cluster has multiple GPUs,
  # multiple GPUs can be utilized by specifying riva.numGpus=<num_gpus>
  # and distributing models to across GPUs by specifying GPU instance
  # numbers along with RMIR name.
  # This will load that particular model only on specified GPU(s).
  # If no GPU is specified, it will load on all available GPUs.
  # GPUs should be comma separated and prefixed by "#". e.g.,
  # rmir_asr_conformer_en_us_str#0,1 will load on GPUs 0 and 1
  # rmir_tts_fastpitch_hifigan_en_us_ipa#2 will load on GPU 2
  # rmir_nlp_punctuation_bert_base_en_us will load on all GPUs
  ngcModelConfigs:
    tritonGroup0:
      enabled: true
      models:
      # asr
      ### Streaming w/ CPU decoder, best throughput configuration
      - nvidia/riva/rmir_asr_conformer_en_us_str_thr:2.14.0
      ### Streaming w/ CPU decoder, best latency configuration
      # - nvidia/riva/rmir_asr_conformer_en_us_str:2.14.0
      ### Offline w/ CPU decoder
      - nvidia/riva/rmir_asr_conformer_en_us_ofl:2.14.0

      ### Speaker diarization model
      # - nvidia/riva/rmir_diarizer_offline:2.14.0

      # nlp
      ### Punctuation model
      - nvidia/riva/rmir_nlp_punctuation_bert_base_en_us:2.14.0

      # tts
      ### Multi-speaker en-US model IPA
      - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us_ipa:2.14.0
      # - nvidia/riva/rmir_tts_radtts_hifigan_en_us_ipa:2.14.0
      ### Multi-speaker en-US model ARPABET
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_en_us:__RIVA_MODEL_VERSION_TMPL_
      ### Multi speaker zh-CN model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_zh_cn_ipa:2.14.0
      ### Multi speaker es-US model IPA
      - nvidia/riva/rmir_tts_fastpitch_hifigan_es_us_ipa:2.14.0
      ### Single speaker es-ES F model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_es_es_f_ipa:2.14.0
      ### Single speaker es-ES M model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_es_es_m_ipa:2.14.0
      ### Single speaker it-IT F model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_it_it_f_ipa:2.14.0
      ### Single speaker it-IT M model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_it_it_m_ipa:2.14.0
      ### Single speaker de-DE M model IPA
      # - nvidia/riva/rmir_tts_fastpitch_hifigan_de_de_m_ipa:2.14.0
      ### Beta emotion mixing model
      # - nvidia/riva/rmir_tts_radttspp_hifigan_en_us_ipa:2.14.0

      # nmt
      ### Bi-lingual NMT models
      # - nvidia/riva/rmir_nmt_de_en_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_de_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_es_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_zh_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_ru_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_fr_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_es_en_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_ru_en_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_zh_en_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_fr_en_24x6:2.14.0
      ### Multi-lingual NMT models
      # - nvidia/riva/rmir_nmt_en_deesfr_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_en_deesfr_12x2:2.14.0
      # - nvidia/riva/rmir_nmt_deesfr_en_24x6:2.14.0
      # - nvidia/riva/rmir_nmt_deesfr_en_12x2:2.14.0
      ### Megatron NMT models
      # - nvidia/riva/rmir_megatronnmt_any_en_500m:2.14.0
      # - nvidia/riva/rmir_megatronnmt_en_any_500m:2.14.0
      # - nvidia/riva/rmir_nmt_megatron_1b_any_en:2.14.0


  # modelDeployVolume
  # Location where the Model Repo Generator should downloads the NGC artifacts
  # and writes the model repository
  # Note: This volume is always mounted as `/data/` in the speech container
  #       and the model repository path is `/data/models`. Extra plugins (for TRT)
  #       that may be required are added to `/data/plugins`
  # with hostpath set, the directory /data/riva needs to exist locally (mkdir -p /data/riva)
  modelDeployVolume:
    #emptyDir: {}
    hostPath:
      type: DirectoryOrCreate
      path: /data/riva
    # nfs:
    #   server: 127.0.0.1
    #   path: /export/rivamodels

persistentVolumeClaim:
  usePVC: false
  # configure storage on your cloud provider and use `storageClassName` for creating PVCs
  storageClassName:
  # configure storage on your cloud provider and use `storageAccessMode` for creating PVCs, by default `ReadWriteOnce`
  storageAccessMode:
  storageSize: 50Gi
  workdirClaimName: riva-workdir-pvc
  # Flag to add annotation "helm.sh/resource-policy: "keep"" to PVC to skip deletion on uninstall
  keepPVC: false

service:
  type: ClusterIP
  # configure `type: ClusterIP` in case of 'nginx'
  # type: ClusterIP
  nodeport: 32222
  # enable headless service additionally
  enableHeadlessService: false

# Optional setup to create an ingress controller and LoadBalancer
# Ingress and LB need to be already installed and setup - this section just configures.
# This example uses traefik (https://metallb.universe.tf/), modify to suit your needs.
ingress:
  # to use a default bare bones ingress controller.
  useIngress: false
  class: traefik
  # should be the fqdn for your service.
  hostname: riva.nvda

  #class: nginx
  # tls secret name need to be provided to allow ingress validation in case `class: nginx`
  tlsSecret:
  # configure if server wants to communicate only certain clients via  `corsOrigin` in case `class: nginx`
  corsOrigin:


# If your installation will expose the service outside of the kubernetes cluster
# you will need to decide how to expose the service. If you use Loadbalancer and
# are not in a cloud service provider your cluster needs some way of connecting a
# service to an IP.
# If you dont want to use a loadbalancer make sure to edit the service type above.
# this section configures, but does not install metallb. (https://metallb.universe.tf/)
loadbalancer:
  # false - do nothing.  True sets up the ipRange as allocatable IP's to services.
  useMetalLB: false
  # the range of IP's available to your cluster. (ipRange: 192.168.1.240-192.168.1.250)
  ipRange: 10.42.0.190-10.42.0.192
